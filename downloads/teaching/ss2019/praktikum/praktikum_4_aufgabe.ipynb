{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "praktikum_4_aufgabe.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9wLM0LR7sQ3N"
      },
      "source": [
        "# Softwaretechnologien für Natürlichsprachliche System\n",
        "---\n",
        "Sommersemester 2019\n",
        "\n",
        "Modulverantwortliche: Prof. Dr. Udo Hahn, Sven Büchel\n",
        "\n",
        "Lehrstuhl für Computerlingustik \n",
        "\n",
        "Friedrich-Schiller-Universität Jena\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Oq1WYezetLS8"
      },
      "source": [
        "**Blatt 4 vom 20. Juni 2019: Datenanalyse**\n",
        "\n",
        "Abgabe bis Dienstag, den **25. Juni 2019**, 23:59\n",
        "\n",
        "Per Email an **johannes.hellrich@uni-jena.de**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aFejKkIct3V4"
      },
      "source": [
        "**Vorname:** ...\n",
        "\n",
        "** Name:** ...\n",
        "\n",
        "** Matrikelnummer:** ...\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JkdZD5BWBy5",
        "colab_type": "text"
      },
      "source": [
        "**Erreichte Punktzahl: .../10**\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybek346bP7j5",
        "colab_type": "text"
      },
      "source": [
        "## Aufgabe 1: Textprozessierung mit NLTK\n",
        "**(... /2 Punkten)**\n",
        "\n",
        "Das Beispiel zeigt, wie ein getaggtes Korpus mit NLTK geladen und eine Statistik zu den darin enthaltenen Wortarten erstellt werden kann. In der Aufgabe soll das gleiche Korpus ohne Annotationen mit einem in NLTK enthaltenen Tagger POS-annotiert werden. Analysieren Sie das Ergebnis und vergleichen Sie es mit den kanonischen Annotationen aus dem Beispiel, welche Unterschiede fallen Ihnen bei den Wörtern am Anfang und bei den häufigsten POS auf?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJtAW477QjED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "58621125-94af-4dd9-8eaa-a65c1e33423f"
      },
      "source": [
        "import nltk\n",
        "import collections\n",
        "\n",
        "nltk.download('brown') # Installiert Brown Korpus\n",
        "tagged_corpus = nltk.corpus.brown.tagged_words()\n",
        "print(tagged_corpus[:10]) \n",
        "\n",
        "#Statistiken zum Korpus\n",
        "counter = collections.Counter()\n",
        "for word, pos in tagged_corpus:\n",
        "  counter[pos] += 1\n",
        "sorted_counts = counter.most_common(10)\n",
        "print(sorted_counts)\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN')]\n",
            "[('NN', 152470), ('IN', 120557), ('AT', 97959), ('JJ', 64028), ('.', 60638), (',', 58156), ('NNS', 55110), ('CC', 37718), ('RB', 36464), ('NP', 34476), ('VB', 33693), ('VBN', 29186), ('VBD', 26167), ('CS', 22143), ('PPS', 18253), ('VBG', 17893), ('PP$', 16872), ('TO', 14918), ('PPSS', 13802), ('CD', 13510)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBqFEp-dVrZu",
        "colab_type": "text"
      },
      "source": [
        "###Lösung"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwggYmzKTu15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger') #Installiert den NLTK default Tagger\n",
        "untagged_corpus = nltk.corpus.brown.words()\n",
        "#todo, vgl. http://www.nltk.org/book/ch05.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrLwjoQsgxQY",
        "colab_type": "text"
      },
      "source": [
        "#Klassifikation mit NLTK\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpaPjeX2XEWv",
        "colab_type": "text"
      },
      "source": [
        "## Korpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31HiJzQGVlpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('names') #installiert ein \"corpus\" aus Namen mit Geschlechtsangabe\n",
        "print(\"Dateien = Labels:\",nltk.corpus.names.fileids())\n",
        "print(\"Inhalt:\",nltk.corpus.names.words()[:5])\n",
        "ng_list = [(name,gender[:1])\n",
        "           for gender in names.fileids()\n",
        "           for name in names.words(gender)]   \n",
        "print(\"Paare aus (Namen, Label):\",ng_list[:5])\n",
        "\n",
        "print(\"Weibliche Namen:\",sum([1 if x[1]==\"f\" else 0 for x in ng_list]))\n",
        "print(\"Männliche Namen:\",sum([1 if x[1]==\"m\" else 0 for x in ng_list]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0mvDT0wXN9O",
        "colab_type": "text"
      },
      "source": [
        "##Randomisiertes Trainingsmaterial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNS1Vb4CXRSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.shuffle(ng_list)\n",
        "\n",
        "test_data = ng_list[:1000]\n",
        "train_data = ng_list[1000:]\n",
        "\n",
        "print(\"Testdaten:\",test_data[:5])\n",
        "print(\"Trainingsdaten:\",train_data[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTSY9IKDYBL0",
        "colab_type": "text"
      },
      "source": [
        "##Feature Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IRHrk5UYDoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gender_features(word):\n",
        "    features = {}\n",
        "    word = word.lower()\n",
        "    features['first_letter'] = word[0]\n",
        "    return features\n",
        "\n",
        "train_set = nltk.classify.apply_features(gender_features, train_data)\n",
        "test_set = nltk.classify.apply_features(gender_features, test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wOGqw6QYTBr",
        "colab_type": "text"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWjIZM6_YWGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set)) #ca. 0.63"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCldZayje1nF",
        "colab_type": "text"
      },
      "source": [
        "##Fehleranalyse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3WjuUYfe1N4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.show_most_informative_features(5)  #beste Features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAbTuwRVfAdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getErrors(test_set_,classifier, limit=20):\n",
        "    errors = []\n",
        "    for (name, tag) in test_set_:\n",
        "        guess = classifier.classify(gender_features(name))\n",
        "        if guess != tag:\n",
        "            errors.append( (tag, guess, name) )\n",
        "    for (tag, guess, name) in sorted(errors)[:limit]:\n",
        "        print('correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNvr7hDme-3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getErrors(test_data,classifier) #falsch klassifiziert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fvhgh7gSuShj"
      },
      "source": [
        "## Aufgabe 2: Bessere Features\n",
        "**(... /2 Punkten)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7RaUmxWB3LC",
        "colab_type": "text"
      },
      "source": [
        "Erweitern Sie den angegebenen Code durch das hinzufügen weiterer Features um die Genauigkeit des Klassifikators zu erhöhen. Sie sollten Werte von 0.75 oder mehr erreichen können. Beachten Sie dabei auch die Möglichkeiten zur Anzeige der informativsten Features bzw. falsch klassifizierter Wörter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A0Fn7uolNle",
        "colab_type": "text"
      },
      "source": [
        "###Lösung"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4W7zEvJcAMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def your_gender_features(word):\n",
        "    features = {}\n",
        "    word = word.lower()\n",
        "    features['first_letter'] = word[0]\n",
        "    # more features here ...\n",
        "    return features\n",
        "\n",
        "#Features für Daten extrahieren\n",
        "train_set = nltk.classify.apply_features(your_gender_features, train_data)\n",
        "test_set = nltk.classify.apply_features(your_gender_features, test_data)\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(\"Ihre Accuracy:\",nltk.classify.accuracy(classifier, test_set)) # > 0.75?\n",
        "classifier.show_most_informative_features(5) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ95SvqpB3LH",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7VnhcRZB3LL",
        "colab_type": "text"
      },
      "source": [
        "## Aufgabe 3: Kreuzvalidierung\n",
        "\n",
        "**(... / 3 Punkten)**\n",
        "\n",
        "Die Genauigkeit eines Klassifikators ist Abhängig von der genauen Verteilung der Daten im Trainings- und Testmaterial. Um verlässliche Aussagen über die Qualität eines Klassifikators machen zu können, ist es üblich Experimente zu wiederholen. Bei der Kreuzvalidierung wird ein Datenset in n Teilezerlegt, von denen reihum jedes als Testdatensatz fungiert, während die übrigen als Trainingsdaten verwendet werden. Ergänzen Sie den unten angegebenen Code, um eine 5-fache Kreuzvalidierung des Classifiers durchzuführen und ermitteln Sie so seine mittlere Performance und deren Standardabweichung (etwa über die in NumPy eingebauten Funktionen)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5c12fFZkywk",
        "colab_type": "text"
      },
      "source": [
        "###Vorbereitung"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "072l-X7dkFFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('names') #installiert ein \"Korpus\" aus Namen mit Geschlechtsangabe\n",
        "ng_list = [(name,gender[:1])\n",
        "           for gender in names.fileids()\n",
        "           for name in names.words(gender)] \n",
        "random.shuffle(ng_list)\n",
        "\n",
        "def gender_features(word):\n",
        "    features = {}\n",
        "    word = word.lower()\n",
        "    features['first_letter'] = word[0]\n",
        "    return features\n",
        "  \n",
        "#7.5k Wörter mit Label als Daten für den folgenden Vergleich\n",
        "all_data = nltk.classify.apply_features(gender_features, ng_list[:7500])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rREXSIjPB3LN",
        "colab_type": "text"
      },
      "source": [
        "### Lösung\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuQOGIK_lZYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def experiment(train, test):\n",
        "  #todo\n",
        "  return accuracy\n",
        "\n",
        "def get_folds(data, n):\n",
        "  size = len(data) // n # // für Ganzzahl\n",
        "  current = 0\n",
        "  while current < len(data):\n",
        "    #todo\n",
        "    yield test_split, train_splits\n",
        "  \n",
        "accuracies = []\n",
        "for train, test in get_folds(all_data, 5):\n",
        "  accuracies.append(experiment(train, test))\n",
        "  \n",
        "#todo\n",
        "print(\"Accuracy:\")\n",
        "print(\"Standardabweichung:\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYIqFo3mB3LP",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO3PkwDPrCjp",
        "colab_type": "text"
      },
      "source": [
        "#Klassifikation mit sklearn\n",
        "\n",
        "Dieser Abschnitt behandelt sklearn am Beispiel Dokumentenklassifikation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM_jCSJ5xlF3",
        "colab_type": "text"
      },
      "source": [
        "##Korpus laden"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCFEGm_OB3LS",
        "colab_type": "code",
        "outputId": "70abc76a-ae87-42a1-9e68-f6f415e4c38c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "import sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "texts = fetch_20newsgroups(subset='train',\n",
        "                          \n",
        "                             shuffle=True, random_state=42)\n",
        "\n",
        "print(\"Textbeispiele:\")\n",
        "print(\"\\n\".join(texts.data[0].split(\"\\n\")[:3]))\n",
        "print(\"-\"*5)\n",
        "print(\"\\n\".join(texts.data[1].split(\"\\n\")[:3]))\n",
        "print(\"-\"*20)\n",
        "print(\"Alle Labels:\",texts.target)\n",
        "print(\"Label Namen:\",texts.target_names)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Textbeispiele:\n",
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "-----\n",
            "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
            "Subject: SI Clock Poll - Final Call\n",
            "Summary: Final call for SI clock reports\n",
            "--------------------\n",
            "Alle Labels: [7 4 4 ... 3 1 8]\n",
            "Label Namen: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRMtKkNXxpo-",
        "colab_type": "text"
      },
      "source": [
        "##Bag of Word Repräsentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSzbFKcdtgDU",
        "colab_type": "code",
        "outputId": "a5c980be-044c-48d6-c187-eb444c373b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "texts_bow = count_vect.fit_transform(texts.data)\n",
        "index = count_vect.vocabulary_.get(u'car') \n",
        "print(\"Car in 1. Dokument:\",texts_bow[0,index]) \n",
        "print(\"Car in 2. Dokument:\",texts_bow[1,index])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Car in 1. Dokument: 5\n",
            "Car in 2. Dokument: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwxLX77UziYA",
        "colab_type": "text"
      },
      "source": [
        "##Beispielsklassifikator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ8XD7wdB3Lb",
        "colab_type": "code",
        "outputId": "8a9d6784-b6ba-4506-bce5-ae63b460125c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import numpy as np\n",
        "\n",
        "#Training\n",
        "classifier = MultinomialNB().fit(texts_bow, texts.target)\n",
        "\n",
        "#Reklassifikation\n",
        "reclassification = classifier.predict(texts_bow)\n",
        "print(\"Genauigkeit bei Reklassifikation:\",\n",
        "      np.mean(reclassification == texts.target))\n",
        "\n",
        "#Klassifikation neuer Dokumente\n",
        "docs_new = ['Drugs are bad, mmmkay?', 'I want a new and fast GPU!']\n",
        "X_new_counts = count_vect.transform(docs_new)\n",
        "predicted = classifier.predict(X_new_counts)\n",
        "print(\"Label der neuen Dokumente:\",\n",
        "      \", \".join([texts.target_names[label] for label in predicted]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Genauigkeit bei Reklassifikation: 0.9245182959165635\n",
            "Label der neuen Dokumente: talk.politics.misc, sci.crypt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3seLO1Z50ydi",
        "colab_type": "text"
      },
      "source": [
        "##Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2m9IlkK028N",
        "colab_type": "code",
        "outputId": "b8acfab7-5d06-4701-fffd-bda62c0f2d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_texts = fetch_20newsgroups(subset='test',shuffle=True, random_state=42)\n",
        "\n",
        "test_bow = count_vect.transform(test_texts.data)\n",
        "predicted = classifier.predict(test_bow)\n",
        "np.mean(predicted == test_texts.target) \n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7728359001593202"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-x08DT10XSw",
        "colab_type": "text"
      },
      "source": [
        "## Aufgabe 4: Klassifikatorenvergleich\n",
        "\n",
        "**(... / 3 Punkten)**\n",
        "\n",
        "Testen Sie weitere Klassifikatoren in sklearn (mindestens die unten angegebenen) in Hinblick auf ihre Genauigkeit bei der Dokumentenklassifikation. Nutzen Sie dazu die in sklearn enthaltene Kreuzvalidierungsmöglichkeit, nutzen Sie alle Dokumente des Datensatzes und verwenden Sie eine Funktion / Schleife für die Evaluation der Modelle (also kein Copy&Paste Code!). Welcher Klassifikator ist am genauesten?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JW4jBXYiodW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.neighbors\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "to_test=[sklearn.svm.LinearSVC(max_iter=5), \n",
        "         sklearn.naive_bayes.MultinomialNB(), \n",
        "         sklearn.linear_model.LogisticRegression(max_iter=5),\n",
        "         sklearn.neighbors.KNeighborsClassifier()]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hdrGnw45B3Lf",
        "colab_type": "text"
      },
      "source": [
        "### Lösung:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMTi85RdB3Lh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#alle Texte laden und vorverarbeiten statt nur train/test, siehe https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html\n",
        "all_texts_bow #todo \n",
        "all_texts_labels #todo\n",
        "\n",
        "#Kreuzvalidierung einzelner Modelle, Funktion/Schleife erstellen\n",
        "scores = cross_val_score(model_to_test, all_texts_bow, all_texts_labels, cv=5) #5 splits\n",
        "print(scores.mean() , scores.std())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq06-9bTB3Lt",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    }
  ]
}